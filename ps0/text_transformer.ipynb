{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e863cc",
   "metadata": {},
   "source": [
    "!pip install -q torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb3c0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random, math, os, textwrap\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb65fe6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt\n",
    "\n",
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Corpus length (chars):\", len(text))\n",
    "print(\"Sample:\")\n",
    "print(text[:500])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Chars:\", chars)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "def encode(s: str):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(idx_list):\n",
    "    return \"\".join(itos[i] for i in idx_list)\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"Encoded data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328c0fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 128  \n",
    "batch_size = 64    \n",
    "\n",
    "def get_batch(split=\"train\"):\n",
    "    src = train_data if split == \"train\" else val_data\n",
    "    # random sample initialization\n",
    "    ix = torch.randint(0, len(src) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([src[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([src[i+1:i+block_size+1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# sanity check\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"Batch x shape:\", xb.shape)  # (B, T)\n",
    "print(\"Batch y shape:\", yb.shape)\n",
    "print(\"Example x (decoded):\")\n",
    "print(decode(xb[0].cpu().tolist()))\n",
    "print(\"Example y (decoded):\")\n",
    "print(decode(yb[0].cpu().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af78b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, block_size=128):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Q, K, V projection\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # causal mask: shape (1, 1, T, T)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        # register buffer \n",
    "        self.register_buffer(\"causal_mask\", mask.view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, d_model), T <= block_size\n",
    "        return: (B, T, d_model)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        H = self.num_heads\n",
    "        Hd = self.head_dim\n",
    "\n",
    "        # Q, K, V: (B, T, D)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        Q = Q.view(B, T, H, Hd).transpose(1, 2)\n",
    "        K = K.view(B, T, H, Hd).transpose(1, 2)\n",
    "        V = V.view(B, T, H, Hd).transpose(1, 2)\n",
    "\n",
    "        # attentions: (B, H, T, T)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Hd)\n",
    "\n",
    "        # causal_mask: (1,1,block_size,block_size) -> next T\n",
    "        mask = self.causal_mask[:, :, :T, :T]  # (1,1,T,T)\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "\n",
    "        # softmax \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # each head (B, H, T, Hd)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # combine heads: (B, H, T, Hd) -> (B, T, D)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, D)\n",
    "\n",
    "        out = self.W_o(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341c3ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-LN \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, block_size=128):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout=dropout, block_size=block_size)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention + residual\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        # FFN + residual\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a159d26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CharTransformerLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=128,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        d_ff=512,\n",
    "        block_size=128,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # token embedding + position embedding\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Multiple TransformerBlock\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout=dropout, block_size=block_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # LayerNorm + output to vocab\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        # some initialization\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) \n",
    "        targets: (B, T)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size, \"Sequence length exceeds block_size\"\n",
    "\n",
    "        # token + position embedding\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        pos = pos.unsqueeze(0)  # (1, T)\n",
    "        tok_emb = self.token_embed(idx)       # (B, T, d_model)\n",
    "        pos_emb = self.pos_embed(pos)         # (1, T, d_model)\n",
    "        x = tok_emb + pos_emb                 # (B, T, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # TransformerBlocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # LN + linearhead\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits_flat = logits.view(-1, self.vocab_size)\n",
    "            targets_flat = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=100, temperature=1.0):\n",
    "        \"\"\"\n",
    "        idx: (B, T) \n",
    "        return: (B, T+max_new_tokens)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        B, T = idx.shape\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)  # (B, T_cond, vocab_size)\n",
    "            logits = logits[:, -1, :]   # (B, vocab_size)\n",
    "\n",
    "            # temperature\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
    "\n",
    "            # next token\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # concat\n",
    "            idx = torch.cat([idx, next_idx], dim=1)  # (B, T+1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5c1ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def generate_text(prompt=\"JULIET:\", max_new_tokens=400, temperature=0.8):\n",
    "    # prompt to tensor\n",
    "    start_ids = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "\n",
    "    sample_ids = model.generate(\n",
    "        start_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    sample_text = decode(sample_ids[0].cpu().tolist())\n",
    "    return sample_text\n",
    "\n",
    "generated = generate_text(\"JULIET:\", max_new_tokens=500, temperature=0.8)\n",
    "print(\"=== Generated Text ===\")\n",
    "print(generated)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
